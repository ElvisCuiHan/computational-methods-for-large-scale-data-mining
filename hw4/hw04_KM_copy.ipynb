{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat 257 Homework 4\n",
    "\n",
    "**Due May 28 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.6.0\n",
      "Commit f9720dc2eb (2021-03-24 12:55 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin19.6.0)\n",
      "  CPU: Apple M1\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-11.0.1 (ORCJIT, westmere)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we continue with the linear mixed effects model (LMM) considered in HW2\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma}_i + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effects predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effects predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma}_i \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$.\n",
    "\n",
    "The log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ is \n",
    "$$\n",
    "    \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = - \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Omega}_i - \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta}),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    \\boldsymbol{\\Omega}_i = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\mathbf{L} \\mathbf{L}^T \\mathbf{Z}_i^T,\n",
    "$$\n",
    "Because the variance component parameter $\\boldsymbol{\\Sigma}$ has to be positive semidefinite, we use its Cholesky factor $\\mathbf{L}$ as optimization variable. \n",
    "\n",
    "Given $m$ independent data points $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$, $i=1,\\ldots,m$, we seek the maximum likelihood estimate (MLE) by maximizing the log-likelihood\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2) = \\sum_{i=1}^m \\ell_i(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2).\n",
    "$$\n",
    "In this assignment, we use the nonlinear programming (NLP) approach for optimization. In HW5, we will derive a EM (expectation-maximization) algorithm for the same problem. \n",
    "\n",
    "IN HW2 we know how to evaluate it efficiently. \n",
    "So we need a function to calculate / evaluate the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages; make sure install them first\n",
    "using BenchmarkTools, CSV, DataFrames, DelimitedFiles, Distributions, Ipopt, LinearAlgebra, \n",
    "MathProgBase, MixedModels, NLopt, Random, RCall, Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. (Optional, 30 bonus pts) Derivatives\n",
    "\n",
    "NLP optimization solvers expect users to provide at least a function for evaluating objective value. If users can provide further information such as gradient and Hessian, the NLP solvers will be more stable and converge faster. Automatic differentiation tools are becoming more powerful but cannot apply to all problems yet.\n",
    "\n",
    "1. Show that the gradient of $\\ell_i$ is\n",
    "\\begin{eqnarray*}\n",
    "\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &=& \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i, \\\\\n",
    "\\nabla_{\\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &=& - \\frac{1}{2} \\operatorname{tr} (\\boldsymbol{\\Omega}_i^{-1}) + \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i, \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &=& - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L},\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{r}_i = \\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta}$. \n",
    "\n",
    "Recall L is the cholesky factor of $\\Sigma$. \n",
    "(He is giving us the derivation directly)\n",
    "\n",
    "2. Derive the observed information matrix and the expected (Fisher) information matrix.\n",
    "\n",
    "If you need a refresher on multivariate calculus, my [Biostat 216 lecture notes](https://ucla-biostat216-2019fall.github.io/slides/16-matrixcalc/16-matrixcalc.html) may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (20 pts) Objective and gradient evaluator for a single datum\n",
    "\n",
    "We expand the code from HW2 to evaluate both objective and gradient. I provide my code for HW2 below as a starting point. You do _not_ have to use this code. If your come up faster code, that's even better. \n",
    "\n",
    "\n",
    "He calculates the quadratic part of the log likelihood this way:\n",
    "$$\\frac{-1}{2}(y_i - X_i\\beta)^T(\\sigma^2I_{n_i} + Z_iLL^TZ_i^T)^{-1}(y_i - X_i\\beta) = \\\\\n",
    "\\frac{-1}{2\\sigma^2}\\big[(y_i - X_i\\beta)^T(y_i - X_i\\beta) - (y_i - X_i\\beta)^TZ_iLM^{-1}L^TZ_i(y_i - X_i\\beta)\\big]$$\n",
    "Where $M^{-1} = (\\sigma^2I_q + L^TZ_i^TZ_iL)^{-1}$.\n",
    "\n",
    "$\\Omega_i^{-1} = \\frac{1}{\\sigma^2}\\big[I_{n_i} - Z_iL(\\sigma^2I_q + L^TZ_i^TZ_iL)^{-1}L^TZ_i^T\\big] = \\frac{1}{\\sigma^2}\\big[I_{n_i} - Z_iLM^{-1}L^TZ_i\\big]$\n",
    "\n",
    "using woodbury with $U = Z_iL, B = \\frac{1}{\\sigma^2}I_q, V^T = L^TZ_i^T$. \n",
    "\n",
    "He stores Cholesky of $M$ in `storage_qq`. Recall that $M = LL^T = U^TU$\n",
    "\n",
    "He treats  $M^{-1}L^TZ_i(y_i - X_i\\beta)$ as a triangle solve with $M = U^TU$. Then $U \\hspace{.2cm} / \\hspace{.2cm} L^TZ_i(y_i - X_i\\beta)$ is actually one half of $(y_i - X_i\\beta)^TZ_iLM^{-1}L^TZ_i(y_i - X_i\\beta)$. Then we can just treat it as a dot product! He stores the triangle solve in `storage_q`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LmmObs"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds an LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y          :: Vector{T}\n",
    "    X          :: Matrix{T}\n",
    "    Z          :: Matrix{T}\n",
    "    # arrays for holding gradient\n",
    "    âˆ‡Î²         :: Vector{T}\n",
    "    âˆ‡ÏƒÂ²        :: Vector{T}\n",
    "    âˆ‡Î£         :: Matrix{T}    \n",
    "    # working arrays\n",
    "    # TODO: whatever intermediate arrays you may want to pre-allocate\n",
    "    yty        :: T\n",
    "    xty        :: Vector{T}\n",
    "    zty        :: Vector{T}\n",
    "    storage_p  :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    storage_q2 :: Vector{T}\n",
    "    xtx        :: Matrix{T}\n",
    "    ztx        :: Matrix{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "    ltztzl     :: Matrix{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmObs(y::Vector, X::Matrix, Z::Matrix)\n",
    "\n",
    "Create an LMM datum of type `LmmObs`.\n",
    "\"\"\"\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q    = size(X, 1), size(X, 2), size(Z, 2)    \n",
    "    âˆ‡Î²         = Vector{T}(undef, p)\n",
    "    âˆ‡ÏƒÂ²        = Vector{T}(undef, 1)\n",
    "    âˆ‡Î£         = Matrix{T}(undef, q, q)    \n",
    "    yty        = abs2(norm(y))\n",
    "    xty        = transpose(X) * y\n",
    "    zty        = transpose(Z) * y    \n",
    "    storage_p  = Vector{T}(undef, p)\n",
    "    storage_q  = Vector{T}(undef, q)\n",
    "    storage_q2 = Vector{T}(undef, q)\n",
    "    xtx        = transpose(X) * X\n",
    "    ztx        = transpose(Z) * X\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    ltztzl     = similar(ztz)\n",
    "    LmmObs(y, X, Z, âˆ‡Î², âˆ‡ÏƒÂ², âˆ‡Î£, \n",
    "        yty, xty, zty, storage_p, storage_q, storage_q2,  \n",
    "        xtx, ztx, ztz, storage_qq, ltztzl)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`logl!(obs::LmmObs, Î², L, ÏƒÂ², needgrad=false)`\n",
    "\n",
    "Evaluate the log-likelihood of a single LMM datum at parameter values `Î²`, `L`, \n",
    "and `ÏƒÂ²`. If `needgrad==true`, then `obs.âˆ‡Î²`, `obs.âˆ‡Î£`, and `obs.ÏƒÂ²` are filled \n",
    "with the corresponding gradient. \n",
    "\n",
    "This is how I am computing the gradients. \n",
    "$$\\nabla_{\\boldsymbol{\\beta}} = \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i$$\n",
    "$$ = X_i^T\\big[\\frac{1}{\\sigma^2}(I_{n_i} - Z_iLM^{-1}L^TZ_i^T)\\big](y-X_i\\beta)$$\n",
    "$$ = \\frac{1}{\\sigma^2}\\big[X_i^T(y-X_i\\beta) - X^TZ_iL U \\hspace{.1cm} / \\hspace{.1cm} \\underbrace{U^T \\hspace{.1cm} / \\hspace{.1cm} L^TZ_i^T(y-X_i\\beta)}_{\\texttt{storage_q}}\\big]$$\n",
    "\n",
    "And from the loglikelihood, we already computed $U^T \\hspace{.1cm} / \\hspace{.1cm} L^TZ_i^T(y-X_i\\beta)$. The strategy I deploy is calculating $X_i^T(y-X_i\\beta)$, then solving $U \\hspace{.1cm} / \\hspace{.1cm} \\cdots$ and left multiplying by $X^TZ_iL$. Finally I add and divide the whole thing by $\\frac{1}{\\sigma^2}$. \n",
    "\n",
    "$$\\nabla_{\\sigma^2} = - \\frac{1}{2} \\operatorname{tr} (\\boldsymbol{\\Omega}_i^{-1}) + \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i$$\n",
    "$$ = -\\frac{1}{2} \\big[\\text{Tr}\\big(\\Omega_i^{-1}\\big) - r_i^T\\Omega_i^{-1}\\Omega_i^{-1}r_i\\big]$$\n",
    "$$ = -\\frac{1}{2} \\big[\\text{Tr}\\big(\\frac{1}{\\sigma^2}(I_{n_i} - Z_iLM^{-1}L^TZ_i^T)\\big) - r_i^T[\\frac{1}{\\sigma^2}(I_{n_i} - Z_iLM^{-1}L^TZ_i^T)][\\frac{1}{\\sigma^2}(I_{n_i} - Z_iLM^{-1}L^TZ_i^T)]r_i$$\n",
    "$$ = -\\frac{1}{2} \\big[\\frac{n}{\\sigma^2} - \\frac{1}{\\sigma^2}\\text{Tr}\\big(LM^{-1}L^TZ_i^TZ_i\\big) - \\frac{1}{(\\sigma^2)^2}\\big[\\underbrace{r_i^Tr_i}_{\\texttt{rtr}} - 2\\underbrace{r_i^TZ_iLM^{-1}L^TZ_i^Tr_i}_{\\texttt{qf}} + r_i^TZ_iLM^{-1}L^TZ_i^TZ_iLU / \\underbrace{U^T / L^TZ_i^Tr_i}_{\\texttt{storage_q}} \\big]\\big]$$\n",
    "$$ = -\\frac{1}{2\\sigma^2} \\big[n - \\text{Tr}\\big(LM^{-1}L^TZ_i^TZ_i\\big) - \\frac{1}{\\sigma^2}\\big[\\texttt{rtr} - 2\\texttt{qf} + r_i^TZ_iLM^{-1}L^TZ_i^TZ_iLU / \\texttt{storage_q} \\big]\\big]$$\n",
    "\n",
    "Calculating $LM^{-1}L^TZ_i^TZ_i$ for the trace is beneficial because it is much smaller than the original matrix and we will use it to calculate $\\frac{\\partial}{\\partial \\Sigma}$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mathbf{\\Sigma}} = - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i $$\n",
    "$$ = -\\frac{1}{\\sigma^2}\\big[Z_i^TZ_i - Z_i^TZ_iLM^{-1}L^TZ_i^TZ_i\\big] + Z_i^T\\Omega^{-1}r_i (Z_i^T\\Omega^{-1}r_i)^T $$ \n",
    "Where $Z_i^t\\Omega^{-1}r_i = \\frac{1}{\\sigma^2} [Z_i^Tr_i - Z_i^TZ_iLMâ»Â¹L^TZ_i^Tr_i]$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl! (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### try to work with smaller vectors instead of storage_n\n",
    "function logl!(\n",
    "        obs      :: LmmObs{T}, \n",
    "        Î²        :: Vector{T}, \n",
    "        L        :: Matrix{T}, \n",
    "        ÏƒÂ²       :: T,\n",
    "        needgrad :: Bool = true\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)\n",
    "    ####################\n",
    "    # Evaluate objective\n",
    "    ####################    \n",
    "    \n",
    "    # form the q-by-q matrix: M = ÏƒÂ² * I + Lt Zt Z L\n",
    "    copy!(obs.storage_qq, obs.ztz)\n",
    "    BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.storage_qq) # Lt * ZtZ\n",
    "    BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.storage_qq) # LtZtZ * L\n",
    "    \n",
    "    @inbounds for j in 1:q\n",
    "        obs.storage_qq[j, j] += ÏƒÂ² # add sigma squared to diagonal aka + ÏƒÂ² * I\n",
    "    end\n",
    "    \n",
    "    # cholesky on M = ÏƒÂ² * I + Lt Zt Z L\n",
    "    LAPACK.potrf!('U', obs.storage_qq) # computes and overwrites storage_qq with cholesky of M\n",
    "    \n",
    "    # storage_q = (Mchol.U') \\ (Lt * (Zt * res))\n",
    "    BLAS.gemv!('N', T(-1), obs.ztx, Î², T(1), copy!(obs.storage_q, obs.zty)) # storage_q = Zt(Y-XÎ²)\n",
    "    BLAS.trmv!('L', 'T', 'N', L, obs.storage_q)    # storage_q = Lt * Zt(Y-XÎ²) upper triangle multiply and \n",
    "    BLAS.trsv!('U', 'T', 'N', obs.storage_qq, obs.storage_q) # triangle solve storage_q = U \\ LtZt(Y-XÎ²) \n",
    "    # tri_solve = copy(obs.storage_q) # just checking that it doens't get overwritten, we good. \n",
    "    # but this is only one half (we have to do it again for U^T) if we want the full thing. \n",
    "    \n",
    "    # l2 norm of residual vector (y-xB)'(y-xB)\n",
    "    copy!(obs.storage_p, obs.xty)\n",
    "    rtr  = obs.yty +\n",
    "        dot(Î², BLAS.gemv!('N', T(1), obs.xtx, Î², T(-2), obs.storage_p))\n",
    "    \n",
    "    # assemble pieces\n",
    "    logl::T = n * log(2Ï€) + (n - q) * log(ÏƒÂ²) # constant term\n",
    "    @inbounds for j in 1:q\n",
    "        logl += 2log(obs.storage_qq[j, j])\n",
    "    end\n",
    "    qf    = abs2(norm(obs.storage_q)) # quadratic form term AKA = (y-xb)'ZLM^{-1}LtZ(y-xb)\n",
    "    logl += (rtr - qf) / ÏƒÂ² # (y-xb)'Î©^{-1}(y-xb) / ÏƒÂ²\n",
    "    logl /= -2 # (y-xb)'Î©^{-1}(y-xb) / 2ÏƒÂ²\n",
    "    \n",
    "    ###################\n",
    "    # Evaluate gradient\n",
    "    ###################    \n",
    "    if needgrad  # aka if someone says true then we calculate the gradient \n",
    "        # TODO: fill âˆ‡Î², âˆ‡L, âˆ‡ÏƒÂ² by gradients\n",
    "        # âˆ‡Î² = Xt Î©^{âˆ’1} ð‘Ÿ = 1/ÏƒÂ² [Xt(y-xb) - Xt * Z * L (U^T / storage_q)]\n",
    "        # = 1/ÏƒÂ² [Xty - XtXb - Xt Z L (U^T / storage_q)]  break up residual into already computed parts\n",
    "        \n",
    "        copy!(obs.âˆ‡Î², obs.xty) # put Xty into end vector \n",
    "        BLAS.gemv!('N', T(-1), obs.xtx, Î², T(1), obs.âˆ‡Î²) # âˆ‡Î² = Xty - XtXb\n",
    "        \n",
    "        # now second half Xt * Z * L * (U^t / U / LtZt(y-xb))  (we have ztx already) \n",
    "        # U / LtZt(y-xb) = storage_q\n",
    "        copy!(obs.storage_q2, obs.storage_q)\n",
    "        BLAS.trsv!('U', 'N', 'N', obs.storage_qq, obs.storage_q2) # storage_q2  = U^T / storage_q\n",
    "        BLAS.trmv!('L', 'N', 'N', L, obs.storage_q2) # storage_q2 = L * storage_q2  \n",
    "        BLAS.gemv!('T', T(-1), obs.ztx, obs.storage_q2, T(1), obs.âˆ‡Î²) # - xtz * obs.storage_q2 + âˆ‡Î²\n",
    "        obs.âˆ‡Î² .= (1 / ÏƒÂ²) .* obs.âˆ‡Î² # divide whole thing by ÏƒÂ²\n",
    "        # LMâ»Â¹LtZtr = storage_q2\n",
    "        \n",
    "        # âˆ‡ÏƒÂ² = -1/2[tr(Î©â»Â¹) - rtÎ©â»Â¹Î©â»Â¹r] \n",
    "        # rtÎ©â»Â¹Î©â»Â¹r is partially calculated in log likelihood\n",
    "        # rtÎ©â»Â¹Î©â»Â¹r = (1/ÏƒÂ²)Â² * [rtr - 2rtZLMâ»Â¹LtZtr + rtZLMâ»Â¹LtZtZLMâ»Â¹LtZtr]\n",
    "        # = (1/ÏƒÂ²)Â² * [rtr - 2qf + rtZLMâ»Â¹LtZtZLMâ»Â¹LtZtr] so we need to calculate the last term\n",
    "        # have LMâ»Â¹LtZtr already therefore just need to do transpose(LMâ»Â¹LtZtr)(ZtZ)(LMâ»Â¹LtZtr)\n",
    "        \n",
    "        copy!(obs.storage_q, obs.storage_q2) # just put LMâ»Â¹LtZtr into storage_q\n",
    "        BLAS.gemm!('N', 'N', T(1), obs.ztz, obs.storage_q2, T(0), obs.storage_q)  # ZtZ * LMâ»Â¹LtZtr\n",
    "        obs.âˆ‡ÏƒÂ² .= rtr .- 2qf .+ dot(obs.storage_q2, obs.storage_q) # rtr - 2qf + rtZLMâ»Â¹Lt * ZtZLMâ»Â¹LtZtr\n",
    "        obs.âˆ‡ÏƒÂ² .= (1 / ÏƒÂ²) .* obs.âˆ‡ÏƒÂ² # rtÎ©â»Â¹Î©â»Â¹r * ÏƒÂ²\n",
    "        \n",
    "        # tr(Î©^{âˆ’1}) = tr(1/ÏƒÂ²(In - ZLM^{-1}LtZt)) = n/ÏƒÂ² - 1/ÏƒÂ² tr(ZLM^{-1}LtZt)\n",
    "        # trace is cyclical so tr(ZLM^{-1}LtZt) = tr(LM^{-1}LtZtZ) which is much smaller (q x q)\n",
    "        # want to calculate LMâ»Â¹LtZtZ because it is in trace and âˆ‡L\n",
    "        copy!(obs.ltztzl, obs.ztz)\n",
    "        BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.ltztzl) # L * Z'Z\n",
    "        LAPACK.potrs!('U', obs.storage_qq, obs.ltztzl) # solve Mâ»Â¹L'Z'Z\n",
    "        BLAS.trmm!('L', 'L', 'N', 'N', T(1), L, obs.ltztzl) # L * Mâ»Â¹L'Z'Z\n",
    "        \n",
    "        # âˆ‡ÏƒÂ² = -1/2ÏƒÂ² [n - trace(LMâ»Â¹LtZtZ) - 1/ÏƒÂ²(rtr - 2qf + rtZLMâ»Â¹LtZtZLMâ»Â¹LtZtr)]\n",
    "        obs.âˆ‡ÏƒÂ² .= n .- tr(obs.ltztzl) .- obs.âˆ‡ÏƒÂ² \n",
    "        obs.âˆ‡ÏƒÂ² .= (-1 / 2ÏƒÂ²) .* obs.âˆ‡ÏƒÂ²\n",
    "        \n",
    "        \n",
    "        # âˆ‡Î£ = ZtÎ©â»Â¹rrtÎ©â»Â¹Z âˆ’ Z^TÎ©â»Â¹Z \n",
    "        # âˆ’ZtÎ©â»Â¹Z = -1/ÏƒÂ² [ZtZ - (ZtZ)(LMâ»Â¹LtZtZ)]\n",
    "        copy!(obs.âˆ‡Î£, obs.ztz)\n",
    "        BLAS.gemm!('N', 'N', T(-1), obs.ztz, obs.ltztzl, T(1), obs.âˆ‡Î£) # ZtZ - ZtZ * LMâ»Â¹LtZtZ\n",
    "        obs.âˆ‡Î£ .= (- 1 / ÏƒÂ²) .* obs.âˆ‡Î£ # obs.âˆ‡Î£ = âˆ’ZtÎ©â»Â¹Z\n",
    "        \n",
    "        # ZtÎ©â»Â¹rrtÎ©â»Â¹Z\n",
    "        # ZtÎ©â»Â¹r = 1/ÏƒÂ² [Ztr - (ZtZ)*(LMâ»Â¹LtZtr)] \n",
    "        # recall storage_q = ZtZ * LMâ»Â¹LtZtr \n",
    "        copy!(obs.storage_q2, obs.zty)\n",
    "        BLAS.gemv!('N', T(-1), obs.ztx, Î², T(1), obs.storage_q2) # Z'r = Z'y - Z'XÎ²\n",
    "        obs.storage_q .= obs.storage_q2 .- obs.storage_q  # Z'r - Z'ZLMâ»Â¹L'Z'r\n",
    "        obs.storage_q .= (1 / ÏƒÂ²) .* obs.storage_q # ZtÎ©â»Â¹r is qx1 \n",
    "\n",
    "        BLAS.ger!(1.0, obs.storage_q, obs.storage_q, obs.âˆ‡Î£) # (ZtÎ©â»Â¹r)*(rtÎ©â»Â¹Z) + (âˆ’ZtÎ©â»Â¹Z)\n",
    "        \n",
    "    end    \n",
    "    ###################\n",
    "    # Return\n",
    "    ###################        \n",
    "    return logl\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to test correctness and efficiency of the single datum objective/gradient evaluator here. First generate the same data set as in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "Î²  = [2.0; -1.0; rand(p - 2)]\n",
    "ÏƒÂ² = 1.5\n",
    "Î£  = fill(0.1, q, q) + 0.9I # compound symmetry \n",
    "L  = Matrix(cholesky(Symmetric(Î£)).L)\n",
    "# generate y\n",
    "y  = X * Î² + Z * rand(MvNormal(Î£)) + sqrt(ÏƒÂ²) * randn(n)\n",
    "\n",
    "# form the LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl = logl!(obs, Î², L, ÏƒÂ², true) = -3261.917755918759\n",
      "obs.âˆ‡Î² = [-1.1937579038485637, -20.06810413006264, -12.323942329522518, -5.174641191561894, 28.208475690990344]\n",
      "obs.âˆ‡ÏƒÂ² = [5.501620540890977]\n",
      "obs.âˆ‡Î£ = [0.40732856643411974 -0.10370457194287114 0.9214023976867127; -0.1037045719429481 -0.9907411137746662 -0.021658311185019763; 0.9214023976866843 -0.02165831118500082 -0.5355581080217697]\n"
     ]
    }
   ],
   "source": [
    "@show logl = logl!(obs, Î², L, ÏƒÂ², true)\n",
    "@show obs.âˆ‡Î²\n",
    "@show obs.âˆ‡ÏƒÂ²\n",
    "@show obs.âˆ‡Î£;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will lose all 20 points if following statement throws `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert abs(logl - (-3261.9177559187597)) < 1e-8\n",
    "@assert norm(obs.âˆ‡Î² - [-1.1937579038479573, -20.068104130062466, \n",
    "        -12.323942329522424, -5.174641191562253, 28.208475690990248]) < 1e-8\n",
    "@assert abs(obs.âˆ‡ÏƒÂ²[1] - (5.501620540891395)) < 1e-8\n",
    "@assert norm(obs.âˆ‡Î£ - [0.40732856643442306 -0.10370457194285436 0.9214023976868783; \n",
    "        -0.10370457194285436 -0.9907411137746756 -0.02165831118509598; \n",
    "        0.9214023976868783 -0.02165831118509598 -0.5355581080215592]) < 1e-8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark for evaluating objective only. This is what we did in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     681.291 ns (0.00% GC)\n",
       "  median time:      694.265 ns (0.00% GC)\n",
       "  mean time:        700.385 ns (0.00% GC)\n",
       "  maximum time:     1.352 Î¼s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     151"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark logl!($obs, $Î², $L, $ÏƒÂ², false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark for objective + gradient evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     1.788 Î¼s (0.00% GC)\n",
       "  median time:      1.863 Î¼s (0.00% GC)\n",
       "  mean time:        1.877 Î¼s (0.00% GC)\n",
       "  maximum time:     16.163 Î¼s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_objgrad = @benchmark logl!($obs, $Î², $L, $ÏƒÂ², true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median runt time is 2.1Î¼s. You will get full credit (10 pts) if the median run time is within 10Î¼s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  The points you will get are\n",
    "clamp(10 / (median(bm_objgrad).time / 1e3) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. LmmModel type\n",
    "\n",
    "We create a `LmmModel` type to hold all data points and model parameters. Log-likelihood/gradient of a `LmmModel` object is simply the sum of log-likelihood/gradient of individual data points. \n",
    "\n",
    "Now we have a VECTOR of observations instead of a single point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM model (data + parameters)\n",
    "struct LmmModel{T <: AbstractFloat} <: MathProgBase.AbstractNLPEvaluator\n",
    "    # data\n",
    "    data :: Vector{LmmObs{T}}\n",
    "    # parameters\n",
    "    Î²    :: Vector{T}\n",
    "    L    :: Matrix{T}\n",
    "    ÏƒÂ²   :: Vector{T}    \n",
    "    # arrays for holding gradient\n",
    "    âˆ‡Î²   :: Vector{T}\n",
    "    âˆ‡ÏƒÂ²  :: Vector{T}\n",
    "    âˆ‡L   :: Matrix{T}\n",
    "    # TODO: add whatever intermediate arrays you may want to pre-allocate\n",
    "    xty  :: Vector{T}\n",
    "    ztr  :: Vector{T}\n",
    "    ztr2 :: Vector{T}\n",
    "    xtx  :: Matrix{T}\n",
    "    ztz2 :: Matrix{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmModel(data::Vector{LmmObs})\n",
    "\n",
    "Create an LMM model that contains data and parameters.\n",
    "\"\"\"\n",
    "function LmmModel(obsvec::Vector{LmmObs{T}}) where T <: AbstractFloat\n",
    "    # dims\n",
    "    p    = size(obsvec[1].X, 2)\n",
    "    q    = size(obsvec[1].Z, 2)\n",
    "    # parameters\n",
    "    Î²    = Vector{T}(undef, p)\n",
    "    L    = Matrix{T}(undef, q, q)\n",
    "    ÏƒÂ²   = Vector{T}(undef, 1)    \n",
    "    # gradients\n",
    "    âˆ‡Î²   = similar(Î²)    \n",
    "    âˆ‡ÏƒÂ²  = similar(ÏƒÂ²)\n",
    "    âˆ‡L   = similar(L)\n",
    "    # intermediate arrays\n",
    "    xty  = Vector{T}(undef, p)\n",
    "    ztr  = Vector{T}(undef, abs2(q))\n",
    "    ztr2 = Vector{T}(undef, abs2(q))\n",
    "    xtx  = Matrix{T}(undef, p, p)\n",
    "    ztz2 = Matrix{T}(undef, abs2(q), abs2(q))\n",
    "    LmmModel(obsvec, Î², L, ÏƒÂ², âˆ‡Î², âˆ‡ÏƒÂ², âˆ‡L, xty, ztr, ztr2, xtx, ztz2)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(m::LmmModel, needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of an LMM model at parameter values `m.Î²`, `m.L`, \n",
    "and `m.ÏƒÂ²`. If `needgrad==true`, then `m.âˆ‡Î²`, `m.âˆ‡Î£`, and `m.ÏƒÂ² are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(m::LmmModel{T}, needgrad::Bool = false) where T <: AbstractFloat\n",
    "    logl = zero(T)\n",
    "    if needgrad\n",
    "        fill!(m.âˆ‡Î² , 0)\n",
    "        fill!(m.âˆ‡L , 0)\n",
    "        fill!(m.âˆ‡ÏƒÂ², 0)        \n",
    "    end\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        logl += logl!(obs, m.Î², m.L, m.ÏƒÂ²[1], needgrad)\n",
    "        if needgrad\n",
    "            BLAS.axpy!(T(1), obs.âˆ‡Î², m.âˆ‡Î²)\n",
    "            BLAS.axpy!(T(1), obs.âˆ‡Î£, m.âˆ‡L)\n",
    "            m.âˆ‡ÏƒÂ²[1] += obs.âˆ‡ÏƒÂ²[1]\n",
    "        end\n",
    "    end\n",
    "    # obtain gradient wrt L: m.âˆ‡L = m.âˆ‡L * L   ie keep adding just like he does above\n",
    "    if needgrad\n",
    "       BLAS.trmm!('R', 'L', 'N', 'N', T(1), m.L, m.âˆ‡L)\n",
    "    end\n",
    "    logl\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. (20 pts) Test data\n",
    "\n",
    "Let's generate a fake longitudinal data set to test our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "\n",
    "# dimension\n",
    "m      = 1000 # number of individuals\n",
    "ns     = rand(1500:2000, m) # numbers of observations per individual\n",
    "p      = 5 # number of fixed effects, including intercept\n",
    "q      = 3 # number of random effects, including intercept\n",
    "obsvec = Vector{LmmObs{Float64}}(undef, m)\n",
    "# true parameter values\n",
    "Î²true  = [0.1; 6.5; -3.5; 1.0; 5]\n",
    "ÏƒÂ²true = 1.5\n",
    "Ïƒtrue  = sqrt(ÏƒÂ²true)\n",
    "Î£true  = Matrix(Diagonal([2.0; 1.2; 1.0]))\n",
    "Ltrue  = Matrix(cholesky(Symmetric(Î£true)).L)\n",
    "# generate data\n",
    "for i in 1:m\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    X = Matrix{Float64}(undef, ns[i], p)\n",
    "    X[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), X[:, 2:p])\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    Z = Matrix{Float64}(undef, ns[i], q)\n",
    "    Z[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), Z[:, 2:q])\n",
    "    # generate y\n",
    "    y = X * Î²true .+ Z * (Ltrue * randn(q)) .+ Ïƒtrue * randn(ns[i])\n",
    "    # form a LmmObs instance\n",
    "    obsvec[i] = LmmObs(y, X, Z)\n",
    "end\n",
    "# form a LmmModel instance\n",
    "lmm = LmmModel(obsvec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison with other software, we save the data into a text file `lmm_data.csv`. **Do not put this file in Git.** It takes 246.6MB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(isfile(\"lmm_data.csv\") && filesize(\"lmm_data.csv\") == 244697920) || \n",
    "open(\"lmm_data.csv\", \"w\") do io\n",
    "    p = size(lmm.data[1].X, 2)\n",
    "    q = size(lmm.data[1].Z, 2)\n",
    "    # print header\n",
    "    print(io, \"ID,Y,\")\n",
    "    for j in 1:(p-1)\n",
    "        print(io, \"X\" * string(j) * \",\")\n",
    "    end\n",
    "    for j in 1:(q-1)\n",
    "        print(io, \"Z\" * string(j) * (j < q-1 ? \",\" : \"\\n\"))\n",
    "    end\n",
    "    # print data\n",
    "    for i in eachindex(lmm.data)\n",
    "        obs = lmm.data[i]\n",
    "        for j in 1:length(obs.y)\n",
    "            # id\n",
    "            print(io, i, \",\")\n",
    "            # Y\n",
    "            print(io, obs.y[j], \",\")\n",
    "            # X data\n",
    "            for k in 2:p\n",
    "                print(io, obs.X[j, k], \",\")\n",
    "            end\n",
    "            # Z data\n",
    "            for k in 2:q-1\n",
    "                print(io, obs.Z[j, k], \",\")\n",
    "            end\n",
    "            print(io, obs.Z[j, q], \"\\n\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "Evaluate log-likelihood and gradient of whole data set at the true parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj = logl!(lmm, true) = -2.834275229633783e6\n",
      "lmm.âˆ‡Î² = [11.772120067025336, -1925.6905435286562, -590.4811426626925, 2112.4417700625045, 2261.4604838394553]\n",
      "lmm.âˆ‡ÏƒÂ² = [911.874762056731]\n",
      "lmm.âˆ‡L = [24.419901311354298 24.01192277275017 16.56376848155844; 30.999259003092725 -59.4019026615511 46.58331413472236; 23.424706030628062 51.02946390987342 12.038870789804925]\n"
     ]
    }
   ],
   "source": [
    "copy!(lmm.Î², Î²true)\n",
    "copy!(lmm.L, Ltrue)\n",
    "lmm.ÏƒÂ²[1] = ÏƒÂ²true\n",
    "@show obj = logl!(lmm, true)\n",
    "@show lmm.âˆ‡Î²\n",
    "@show lmm.âˆ‡ÏƒÂ²\n",
    "@show lmm.âˆ‡L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test correctness. You will loss all 20 points if following code throws `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert abs(obj - (-2.8342752296337797e6)) < 1e-6\n",
    "@assert norm(lmm.âˆ‡Î² - [11.772120067073923, -1925.690543528459, \n",
    "        -590.4811426628461, 2112.441770062488, 2261.4604838395016]) < 1e-6\n",
    "@assert norm(lmm.âˆ‡L - [24.419901311377576 24.01192277273847 16.563768481574012; \n",
    "        30.99925900307824 -59.40190266157969 46.58331413470851; \n",
    "        23.424706030649975 51.029463909858194 12.038870789777398]) < 1e-6\n",
    "@assert abs(lmm.âˆ‡ÏƒÂ²[1] - (911.8747620554551)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "Test efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     1.854 ms (0.00% GC)\n",
       "  median time:      1.874 ms (0.00% GC)\n",
       "  mean time:        1.908 ms (0.00% GC)\n",
       "  maximum time:     2.767 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          2616\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_model = @benchmark logl!($lmm, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median run time is 2.122ms. You will get full credit if your median run time is within 10ms. The points you will get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 / (median(bm_model).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "You will lose 1 point for each 100 bytes memory allocation. So the points you will get is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 - median(bm_model).memory / 100, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. (30 pts) Starting point\n",
    "\n",
    "For numerical optimization, a good starting point is critical. Let's start $\\boldsymbol{\\beta}$ and $\\sigma^2$ from the least sqaures solutions (ignoring intra-individual correlations)\n",
    "\\begin{eqnarray*}\n",
    "\\boldsymbol{\\beta}^{(0)} &=& \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{X}_i\\right)^{-1} \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{y}_i\\right) \\\\\n",
    "\\sigma^{2(0)} &=& \\frac{\\sum_i \\|\\mathbf{r}_i^{(0)}\\|_2^2}{\\sum_i n_i} = \\frac{\\sum_i \\|\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta}^{(0)}\\|_2^2}{\\sum_i n_i}.\n",
    "\\end{eqnarray*}\n",
    "To get a reasonable starting point for $\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^T$, we can minimize the least squares criterion (ignoring the noise variance component)\n",
    "$$\n",
    "    \\text{minimize} \\sum_i \\| \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\|_{\\text{F}}^2.\n",
    "$$\n",
    "Derive the minimizer $\\boldsymbol{\\Sigma}^{(0)}$ (10 pts). \n",
    "\n",
    "Frobenius Norm = the square root of the sum of the absolute squares of its elements. So $||A||_F^2 = \\sum_i\\sum_j |a_{ij}|^2$. In addition, $||A||_F^2 = \\text{Trace}(AA^H)$ where $A^H$ is the conjugate transpose. \n",
    "\n",
    "$||r_ir_i^T - Z_i\\Sigma Z_i^T||^2_F = \\text{Trace}((r_ir_i^T - Z_i\\Sigma Z_i^T)^T(r_ir_i^T - Z_i\\Sigma Z_i^T)) = \\text{Trace}(r_ir_i^Tr_ir_i^T - 2Z_i\\Sigma Z_i^Tr_ir_i^T + Z_i\\Sigma Z_i^TZ_i\\Sigma Z_i^T)$\n",
    "\n",
    "$= \\text{Trace}(r_ir_i^Tr_ir_i^T) - \\text{Trace}(2Z_i^Tr_ir_i^TZ_i\\Sigma) + \\text{Trace}(\\Sigma Z_i^TZ_i \\Sigma Z_i^TZ_i)$\n",
    "\n",
    "We want to minimize that quantity, which means taking partial derivatives wrt to the variable ($\\Sigma$) and setting equal to 0. Note that $\\frac{\\partial}{\\partial \\Sigma} \\text{Trace}(A\\Sigma) = A^T$ and $\\frac{\\partial}{\\partial \\Sigma} \\text{Trace}(\\Sigma A\\Sigma B) = B^T\\Sigma^T A^T + A^T \\Sigma^T B^T$\n",
    "\n",
    "$\\Rightarrow \\frac{\\partial}{\\partial \\Sigma} ||r_ir_i^T - Z_i\\Sigma Z_i^T||^2_F = 0 -2Z_i^Tr_ir_i^TZ_i + 2Z_i^TZ_i\\Sigma Z_i^TZ_i$\n",
    "\n",
    "Hence, minimizing $\\sum_i ||r_ir_i^T - Z_i\\Sigma Z_i^T||^2_F$ is solving $\\sum_i -2Z_i^Tr_ir_i^TZ_i + 2Z_i^TZ_i\\Sigma Z_i^TZ_i = 0$ \n",
    "\n",
    "$\\rightarrow \\sum_i Z_i^Tr_ir_i^TZ_i = \\sum_i Z_i^TZ_i\\Sigma Z_i^TZ_i$\n",
    "\n",
    "In order to get an initial value for $\\Sigma$, we need to solve for $\\Sigma$, which we can do by taking the vec. Recall that $\\text{vec}(ABC) = (C^T \\otimes A)\\text{vec}B$, hence \n",
    "$$\\text{vec}\\big(\\sum_i Z_i^Tr_ir_i^TZ_i\\big) = \\text{vec}\\big(\\sum_i \\underbrace{Z_i^TZ_i}_{A}\\underbrace{\\Sigma}_{B} \\underbrace{Z_i^TZ_i}_{C}\\big)$$\n",
    "$$\\sum_i \\big(Z_i^Tr_i \\otimes Z_i^Tr_i\\big) = \\sum_i \\big(Z_i^TZ_i \\otimes Z_i^TZ_i\\big) \\text{vec}(\\Sigma)$$\n",
    "$$\\Rightarrow \\text{vec}(\\Sigma^{(0)}) = \\big(\\sum_i Z_i^TZ_i \\otimes Z_i^TZ_i\\big)^{-1}\\sum_i \\big(Z_i^Tr_i \\otimes Z_i^Tr_i\\big)$$\n",
    "\n",
    "We implement this start point strategy in the function `init_ls()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_ls!"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    init_ls!(m::LmmModel)\n",
    "\n",
    "Initialize parameters of a `LmmModel` object from the least squares estimate. \n",
    "`m.Î²`, `m.L`, and `m.ÏƒÂ²` are overwritten with the least squares estimates.\n",
    "\"\"\"\n",
    "function init_ls!(m::LmmModel{T}) where T <: AbstractFloat\n",
    "    p, q = size(m.data[1].X, 2), size(m.data[1].Z, 2)\n",
    "    # TODO: fill m.Î², m.L, m.ÏƒÂ² by LS estimates\n",
    "    n = length(m.data)\n",
    "    \n",
    "    \n",
    "    fill!(m.Î², 0.0)\n",
    "    fill!(m.L, 0.0)\n",
    "    fill!(m.ÏƒÂ², 0.0)\n",
    "    den_sum = T(0)\n",
    "    \n",
    "    # Î² need to calculate the sum of XitXi and Xityi so add them each time. \n",
    "    for i in 1:n\n",
    "        m.xtx .= m.xtx .+ m.data[i].xtx\n",
    "        m.xty .= m.xty .+ m.data[i].xty\n",
    "        \n",
    "        # need to add all yty to m.ÏƒÂ²\n",
    "        m.ÏƒÂ²[1] += m.data[i].yty\n",
    "        \n",
    "        # keep track of denominator for ÏƒÂ²\n",
    "        den_sum = den_sum + size(m.data[i].X, 1)\n",
    "    end\n",
    "    \n",
    "    mul!(m.Î², m.xtx \\ I, m.xty) # solve inverse, multiply to xty and save to beta\n",
    "   \n",
    "    # ÏƒÂ² need to sum over all residuals and sum of all datapoints. \n",
    "    m.ÏƒÂ²[1] += dot(m.Î², BLAS.gemv!('N', T(1), m.xtx, m.Î², T(-2), m.xty))\n",
    "    m.ÏƒÂ² ./= den_sum # divide by sum of n_i\n",
    "    \n",
    "    # Î£ min = (sum ZtZ kron ZtZ)^{-1}*(sum Ztr kron Ztr)\n",
    "    fill!(m.ztr2, 0.0)\n",
    "    fill!(m.ztz2, 0.0)\n",
    "    fill!(m.ztr, 0.0)\n",
    "    # so I will calculate ZtZ kron ZtZ for each i and update for each person\n",
    "\n",
    "    m.xtx .= T(0)\n",
    "    m.xty .= T(0)\n",
    "    obs.storage_qq .= T(0)\n",
    "    for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        m.ztz2 .+= kron(obs.ztz, obs.ztz) # get Z'Z kron Z'Z\n",
    "        \n",
    "        # need to calculate ztr for each person first      \n",
    "        BLAS.gemv!('N', T(1), obs.ztx, m.Î², T(0), obs.storage_q) # ZtxÎ²\n",
    "        BLAS.axpby!(T(1), obs.zty, T(-1), obs.storage_q) #  Ztr = Zty - ZtxÎ²\n",
    "        \n",
    "        m.ztr2 .+= kron(obs.storage_q, obs.storage_q) # Ztr kron Ztr \n",
    "    end\n",
    "\n",
    "    LAPACK.potrf!('L', m.ztz2) # computes and overwrites with cholesky\n",
    "    LAPACK.potrs!('L', m.ztz2, m.ztr2) # solve (ZtZ kron ZtZ)^{-1}*(Ztr kron Ztr)\n",
    "    # LAPACK.posv!('L', m.ztz2, m.ztr2) # (ZtZ kron ZtZ)^{-1}*(Ztr kron Ztr)\n",
    "    # now \"unvec\" which is reshape\n",
    "    m.L .= reshape(m.ztr2, q, q)\n",
    "    LAPACK.potrf!('L', m.L) # computes and overwrites with cholesky\n",
    "\n",
    "    m\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl!(lmm) = -3.3529912271664594e6\n",
      "lmm.Î² = [0.12814248060847544, 6.497945035096693, -3.5021387747701356, 1.0026609224782135, 5.002697971721907]\n",
      "lmm.ÏƒÂ² = [5.70170988595954]\n",
      "lmm.L = [1.442089600524507 0.055234925755674846 0.04021508338255691; 0.03830200684866265 1.0546394793870688 0.06543803114908463; 0.027886674564416906 0.0610349951874949 1.0014753586943541]\n"
     ]
    }
   ],
   "source": [
    "lmm = LmmModel(obsvec)\n",
    "init_ls!(lmm)\n",
    "@show logl!(lmm)\n",
    "@show lmm.Î²\n",
    "@show lmm.ÏƒÂ²\n",
    "# @show size(lmm.ztr2)\n",
    "# @show lmm.ztr2\n",
    "@show lmm.L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "Your start points should have a log-likelihood larger than -3.352991e6 (10 pts). The points you get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "(logl!(lmm) >  -3.353e6) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "The start point should be computed quickly. Otherwise there is no point using it as a starting point. You get full credit (10 pts) if the median run time is within 1ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  1.12 MiB\n",
       "  allocs estimate:  8007\n",
       "  --------------\n",
       "  minimum time:     384.875 Î¼s (0.00% GC)\n",
       "  median time:      516.042 Î¼s (0.00% GC)\n",
       "  mean time:        597.677 Î¼s (19.23% GC)\n",
       "  maximum time:     14.459 ms (95.60% GC)\n",
       "  --------------\n",
       "  samples:          8322\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_init = @benchmark init_ls!($lmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_init).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. NLP via MathProgBase.jl\n",
    "\n",
    "We define the NLP problem using the modelling tool MathProgBase.jl. Start-up code is given below. Modify if necessary to accomodate your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    fit!(m::LmmModel, solver=Ipopt.IpoptSolver(print_level=5))\n",
    "\n",
    "Fit an `LmmModel` object by MLE using a nonlinear programming solver. Start point \n",
    "should be provided in `m.Î²`, `m.ÏƒÂ²`, `m.L`.\n",
    "\"\"\"\n",
    "function fit!(\n",
    "        m::LmmModel,\n",
    "        solver=Ipopt.IpoptSolver(print_level=5)\n",
    "    )\n",
    "    p    = size(m.data[1].X, 2)\n",
    "    q    = size(m.data[1].Z, 2)\n",
    "    npar = p + ((q * (q + 1)) >> 1) + 1\n",
    "    optm = MathProgBase.NonlinearModel(solver)\n",
    "    # set lower bounds and upper bounds of parameters\n",
    "    # diagonal entries of Cholesky factor L should be >= 0\n",
    "    lb   = fill(-Inf, npar)\n",
    "    ub   = fill( Inf, npar)\n",
    "    offset = p + 1\n",
    "    for j in 1:q, i in j:q\n",
    "        i == j && (lb[offset] = 0)\n",
    "        offset += 1\n",
    "    end\n",
    "    # ÏƒÂ² should be >= 0\n",
    "    lb[end] = 0\n",
    "    MathProgBase.loadproblem!(optm, npar, 0, lb, ub, Float64[], Float64[], :Max, m)\n",
    "    # starting point\n",
    "    par0 = zeros(npar)\n",
    "    modelpar_to_optimpar!(par0, m)\n",
    "    MathProgBase.setwarmstart!(optm, par0)\n",
    "    # optimize\n",
    "    MathProgBase.optimize!(optm)\n",
    "    optstat = MathProgBase.status(optm)\n",
    "    optstat == :Optimal || @warn(\"Optimization unsuccesful; got $optstat\")\n",
    "    # update parameters and refresh gradient\n",
    "    optimpar_to_modelpar!(m, MathProgBase.getsolution(optm))\n",
    "    logl!(m, true)\n",
    "    m\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    modelpar_to_optimpar!(par, m)\n",
    "\n",
    "Translate model parameters in `m` to optimization variables in `par`.\n",
    "\"\"\"\n",
    "function modelpar_to_optimpar!(\n",
    "        par :: Vector,\n",
    "        m   :: LmmModel\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # Î²\n",
    "    copyto!(par, m.Î²)\n",
    "    # L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        par[offset] = m.L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # ÏƒÂ²\n",
    "    par[end] = m.ÏƒÂ²[1]\n",
    "    par\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "\n",
    "Translate optimization variables in `par` to the model parameters in `m`.\n",
    "\"\"\"\n",
    "function optimpar_to_modelpar!(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # Î²\n",
    "    copyto!(m.Î², 1, par, 1, p)\n",
    "    # L\n",
    "    fill!(m.L, 0)\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        m.L[i, j] = par[offset]\n",
    "        offset   += 1\n",
    "    end\n",
    "    # ÏƒÂ²\n",
    "    m.ÏƒÂ²[1] = par[end]    \n",
    "    m\n",
    "end\n",
    "\n",
    "function MathProgBase.initialize(\n",
    "        m                  :: LmmModel, \n",
    "        requested_features :: Vector{Symbol}\n",
    "    )\n",
    "    for feat in requested_features\n",
    "        if !(feat in [:Grad])\n",
    "            error(\"Unsupported feature $feat\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "MathProgBase.features_available(m::LmmModel) = [:Grad]\n",
    "\n",
    "function MathProgBase.eval_f(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "    logl!(m, false) # don't need gradient here\n",
    "end\n",
    "\n",
    "function MathProgBase.eval_grad_f(\n",
    "        m    :: LmmModel, \n",
    "        grad :: Vector, \n",
    "        par  :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    optimpar_to_modelpar!(m, par) \n",
    "    obj = logl!(m, true)\n",
    "    # gradient wrt Î²\n",
    "    copyto!(grad, m.âˆ‡Î²)\n",
    "    # gradient wrt L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        grad[offset] = m.âˆ‡L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # gradient with respect to ÏƒÂ²\n",
    "    grad[end] = m.âˆ‡ÏƒÂ²[1]\n",
    "    # return objective\n",
    "    obj\n",
    "end\n",
    "\n",
    "MathProgBase.eval_g(m::LmmModel, g, par) = nothing\n",
    "MathProgBase.jac_structure(m::LmmModel) = Int[], Int[]\n",
    "MathProgBase.eval_jac_g(m::LmmModel, J, par) = nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. (20 pts) Test drive\n",
    "\n",
    "Now we can run our NLP solver to compute the MLE. For grading purpose, we first use the `:LD_LBFGS` (limited-memory BFGS) algorithm in NLopt.jl here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective value at starting point: -3.3529912271664594e6\n",
      "\n",
      "  0.601676 seconds (1.41 M allocations: 77.586 MiB, 2.44% gc time, 81.84% compilation time)\n",
      "objective value at solution: -2.834264502365345e6\n",
      "\n",
      "solution values:\n",
      "lmm.Î² = [0.12388537837783552, 6.4983373084688845, -3.500513045122987, 1.001821979786551, 5.001953759134669]\n",
      "lmm.ÏƒÂ² = [1.5023521283251702]\n",
      "lmm.L * transpose(lmm.L) = [2.0684734074012767 0.05298548776562313 0.03270542467962338; 0.05298548776562313 1.1218121650879602 0.05601741940842158; 0.03270542467962338 0.05601741940842158 1.012045075975027]\n",
      "gradient @ solution:\n",
      "lmm.âˆ‡Î² = [-0.02076862485362628, -0.026328579871236713, -0.010670280017588851, -0.02573390057455427, -0.0515042935562362]\n",
      "lmm.âˆ‡ÏƒÂ² = [-0.023120619402732245]\n",
      "lmm.âˆ‡L = [0.029564234445700145 0.05924166207970372 0.005205834926730552; 0.07920760965309105 -0.01359965507218792 -0.02132915470149322; 0.006885944413642297 -0.021988023206651264 0.00944892680614832]\n",
      "sqrt(abs2(norm(lmm.âˆ‡Î²)) + abs2(norm(lmm.âˆ‡ÏƒÂ²) + abs2(norm(LowerTriangular(lmm.âˆ‡L))))) = 0.07428917290028683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07428917290028683"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize from least squares\n",
    "init_ls!(lmm)\n",
    "println(\"objective value at starting point: \", logl!(lmm)); println()\n",
    "\n",
    "@time fit!(lmm, NLopt.NLoptSolver(algorithm =:LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000));\n",
    "\n",
    "println(\"objective value at solution: \", logl!(lmm)); println()\n",
    "println(\"solution values:\")\n",
    "@show lmm.Î²\n",
    "@show lmm.ÏƒÂ²\n",
    "@show lmm.L * transpose(lmm.L)\n",
    "println(\"gradient @ solution:\")\n",
    "@show lmm.âˆ‡Î²\n",
    "@show lmm.âˆ‡ÏƒÂ²\n",
    "@show lmm.âˆ‡L\n",
    "@show sqrt(abs2(norm(lmm.âˆ‡Î²)) + abs2(norm(lmm.âˆ‡ÏƒÂ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.âˆ‡L)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "You get 10 points if the following code does not throw `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective at solution should be close enough to the optimal\n",
    "@assert logl!(lmm) > -2.83427e6\n",
    "# gradient at solution should be small enough\n",
    "@assert sqrt(abs2(norm(lmm.âˆ‡Î²)) + abs2(norm(lmm.âˆ‡ÏƒÂ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.âˆ‡L))))) < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "My median run time is 132.5ms. You get 10 points if your median time is within 1s(=1000ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  11.14 KiB\n",
       "  allocs estimate:  234\n",
       "  --------------\n",
       "  minimum time:     102.437 ms (0.00% GC)\n",
       "  median time:      103.190 ms (0.00% GC)\n",
       "  mean time:        104.023 ms (0.00% GC)\n",
       "  maximum time:     108.357 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          48\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_bfgs = @benchmark fit!($lmm, $(NLopt.NLoptSolver(algorithm=:LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval = 10000))) setup = (init_ls!(lmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_bfgs).time / 1e9) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. (10 pts) Gradient free vs gradient-based methods\n",
    "\n",
    "Advantage of using a modelling tool such as MathProgBase.jl is that we can easily switch the backend solvers. For a research problem, we never know beforehand which solver works best. \n",
    "\n",
    "Try different solvers in the NLopt.jl and Ipopt.jl packages. Compare the results in terms run times (the shorter the better), objective values at solution (the larger the better), and gradients at solution (closer to 0 the better). Summarize what you find.\n",
    "\n",
    "See this [page](https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/) for the descriptions of algorithms in NLopt.\n",
    "\n",
    "Documentation for the Ipopt can be found [here](https://coin-or.github.io/Ipopt/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vector of solvers to compare\n",
    "solvers = [\n",
    "    # NLopt: gradient-based algorithms\n",
    "    NLopt.NLoptSolver(algorithm=:LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12,\n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12,\n",
    "        maxeval=10000),\n",
    "    NLopt.NLoptSolver(algorithm=:LD_MMA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000),\n",
    "    # NLopt: gradient-free algorithms\n",
    "    NLopt.NLoptSolver(algorithm=:LN_BOBYQA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000),\n",
    "    # Ipopt\n",
    "    Ipopt.IpoptSolver(print_level=0)\n",
    "]\n",
    "# containers for results\n",
    "runtime = zeros(length(solvers))\n",
    "objvals = zeros(length(solvers))\n",
    "gradnrm = zeros(length(solvers))\n",
    "\n",
    "for (i, solver) in enumerate(solvers)\n",
    "    bm = @benchmark fit!($lmm, $solver) setup = (init_ls!(lmm))\n",
    "    runtime[i] = median(bm).time / 1e9\n",
    "    objvals[i] = logl!(lmm, true)\n",
    "    gradnrm[i] = sqrt(abs2(norm(lmm.âˆ‡Î²)) + abs2(norm(lmm.âˆ‡ÏƒÂ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.âˆ‡L)))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Runtime</th><th>Objective</th><th>Gradnorm</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>4 rows Ã— 3 columns</p><tr><th>1</th><td>0.10316</td><td>-2.83426e6</td><td>0.0742892</td></tr><tr><th>2</th><td>0.129087</td><td>-2.83426e6</td><td>0.00527495</td></tr><tr><th>3</th><td>0.0448093</td><td>-2.83539e6</td><td>31729.5</td></tr><tr><th>4</th><td>5.55828</td><td>-2.83426e6</td><td>1.2764e-5</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& Runtime & Objective & Gradnorm\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.10316 & -2.83426e6 & 0.0742892 \\\\\n",
       "\t2 & 0.129087 & -2.83426e6 & 0.00527495 \\\\\n",
       "\t3 & 0.0448093 & -2.83539e6 & 31729.5 \\\\\n",
       "\t4 & 5.55828 & -2.83426e6 & 1.2764e-5 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4Ã—3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0mâ”‚\u001b[1m Runtime   \u001b[0m\u001b[1m Objective  \u001b[0m\u001b[1m Gradnorm       \u001b[0m\n",
       "\u001b[1m     \u001b[0mâ”‚\u001b[90m Float64   \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64        \u001b[0m\n",
       "â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
       "   1 â”‚ 0.10316    -2.83426e6      0.0742892\n",
       "   2 â”‚ 0.129087   -2.83426e6      0.00527495\n",
       "   3 â”‚ 0.0448093  -2.83539e6  31729.5\n",
       "   4 â”‚ 5.55828    -2.83426e6      1.2764e-5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(Runtime = runtime, Objective = objvals, Gradnorm = gradnrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. (10 pts) Compare with existing art\n",
    "\n",
    "Let's compare our method with lme4 package in R and MixedModels.jl package in Julia. Both lme4 and MixedModels.jl are developed mainly by Doug Bates. Summarize what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method  = [\"257\", \"lme4\", \"MixedModels.jl\"]\n",
    "runtime = zeros(3)  # record the run times\n",
    "loglike = zeros(3); # record the log-likelihood at MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_257 = @benchmark fit!($lmm, $(NLopt.NLoptSolver(algorithm=:LD_MMA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000))) setup=(init_ls!(lmm))\n",
    "runtime[1] = (median(bm_257).time) / 1e9\n",
    "loglike[1] = logl!(lmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lme4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    "library(lme4)\n",
    "library(readr)\n",
    "library(magrittr)\n",
    "\n",
    "testdata <- read_csv(\"lmm_data.txt\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    "rtime <- system.time(mmod <- \n",
    "  lmer(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID), testdata, REML = FALSE))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    "rtime <- rtime[\"elapsed\"]\n",
    "summary(mmod)\n",
    "rlogl <- logLik(mmod)\n",
    "\"\"\"\n",
    "runtime[2] = @rget rtime\n",
    "loglike[2] = @rget rlogl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MixedModels.jl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = CSV.File(\"lmm_data.txt\", types = Dict(1=>String)) |> DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mj = fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), testdata)\n",
    "bm_mm = @benchmark fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), testdata)\n",
    "loglike[3] = loglikelihood(mj)\n",
    "runtime[3] = median(bm_mm).time / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bm_mm)\n",
    "mj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame(method = method, runtime = runtime, logl = loglike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. Be proud of yourself\n",
    "\n",
    "Go to your resume/cv and claim you have experience performing analysis on complex longitudinal data sets with millions of records. And you beat current software by XXX fold. "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
